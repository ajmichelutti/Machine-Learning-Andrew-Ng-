{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as op\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('ex4data1.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X is a 5000 x 400 matrix. Each row is an \"unrolled\" 20 x 20 pixel image of a digit\n",
    "# using numbers to expresses the image's greyscale value at that pixel. \n",
    "\n",
    "X = mat['X']\n",
    "\n",
    "# y is a 5000 x 1 vector containing the correct label for each example in the training set.\n",
    "# the number 0 is given the label 10 in this data.\n",
    "\n",
    "y = mat['y']\n",
    "\n",
    "y = y.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    # Activation function.\n",
    "    \n",
    "    return 1/(1+np.exp(np.negative(z)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    \n",
    "    # Derivative of activation function.\n",
    "    \n",
    "    return sigmoid(z) * (1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_bias_feature(X):\n",
    "    \n",
    "    # Adds bias feature of 1's to X\n",
    "    \n",
    "    m,n = np.shape(X)\n",
    "    X_0 = np.ones(len(X)).reshape(m,1)\n",
    "    \n",
    "    return np.append(X_0, X, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pack_theta(t1, t2):\n",
    "    \n",
    "    # Packs two theta matricies to one long theta vector.\n",
    "    \n",
    "    return np.concatenate((t1.ravel(), t2.ravel()))\n",
    "\n",
    "def unpack_theta(theta_vec, input_layer_size, hidden_layer_size, num_labels):\n",
    "    \n",
    "    # Rearranges theta vector into 2 matrices.\n",
    "    \n",
    "    t1_start = 0\n",
    "    t1_end = hidden_layer_size * (input_layer_size + 1)\n",
    "    t1 = theta_vec[t1_start:t1_end].reshape((hidden_layer_size, input_layer_size + 1))\n",
    "    t2 = theta_vec[t1_end:].reshape((num_labels, hidden_layer_size + 1))\n",
    "    \n",
    "    return t1, t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_theta(L_out, L_in):\n",
    "    \n",
    "    # Create initial weight matrix with values between +/- epsilon\n",
    "    \n",
    "    epsilon = 0.12\n",
    "    \n",
    "    return np.random.rand(L_out, L_in + 1) * (2*epsilon) - epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_function(theta_vec, X, y, input_layer_size, hidden_layer_size, num_labels, lambda_):\n",
    "    \n",
    "    # NN cost function. \n",
    "    # Returns cost and gradient.\n",
    "    \n",
    "    # Reformat theta vector into weight matrices\n",
    "    theta1, theta2 = unpack_theta(theta_vec, input_layer_size, hidden_layer_size, num_labels)\n",
    "    \n",
    "    m = len(X)\n",
    "    \n",
    "    # Forward propagation\n",
    "    a1 = add_bias_feature(X)\n",
    "    z2 = theta1.dot(a1.T)\n",
    "    a2 = sigmoid(z2.T)\n",
    "    a2 = add_bias_feature(a2)\n",
    "    z3 = theta2.dot(a2.T)\n",
    "    a3 = sigmoid(z3.T)\n",
    "    \n",
    "    cost = sum(sum((1/m) * (-y * np.log(a3) - (1-y) * np.log(1-a3)))) + \\\n",
    "            + (lambda_/(2*m)) * (sum(sum(theta1[:,1:]**2)) + sum(sum(theta2[:,1:]**2)))\n",
    "        \n",
    "    theta2_grad = np.zeros(theta2.shape)\n",
    "    theta1_grad = np.zeros(theta1.shape)\n",
    "    \n",
    "    delta_1 = np.zeros(theta1.shape)\n",
    "    delta_2 = np.zeros(theta2.shape)\n",
    "    \n",
    "    # Back Propagation\n",
    "    for i in range(m):\n",
    "        # Error for activation layer 3\n",
    "        d3 = a3[i, :] - y[i, :]\n",
    "        # Error for activation layer 2\n",
    "        d2 = theta2[:,1:].T.dot(d3) * sigmoid_gradient(z2[:,i])\n",
    "        # Accumulate gradient\n",
    "        delta_2 += np.outer(d3, a2[i,:])\n",
    "        delta_1 += np.outer(d2, a1[i,:])\n",
    "    \n",
    "    theta1_grad[:, 0] = delta_1[:, 0] / m\n",
    "    theta1_grad[:, 1:] = delta_1[:, 1:] / m + (theta1[:, 1:] * (lambda_/m))\n",
    "    theta2_grad[:, 0] = delta_2[:, 0] / m\n",
    "    theta2_grad[:, 1:] = delta_2[:, 1:] / m + (theta2[:, 1:] * (lambda_/m))\n",
    "\n",
    "    grad = pack_theta(theta1_grad, theta2_grad)\n",
    "    \n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_numerical_gradient(J, theta):\n",
    "    \n",
    "    # Uses 2 sided difference to approximate the derivative of the cost function\n",
    "    # in order to evaluate the implementation of back propagation.\n",
    "    \n",
    "    epsilon = 10e-4\n",
    "    num_grad = []\n",
    "    perturb = np.zeros(len(theta))\n",
    "    \n",
    "    for i in range(len(theta)):\n",
    "        perturb[i] = epsilon\n",
    "        theta_plus = theta\n",
    "        theta_minus = theta\n",
    "        theta_plus = theta_plus + perturb\n",
    "        theta_minus = theta_minus - perturb\n",
    "        \n",
    "        loss1, _ = J(theta_minus)\n",
    "        loss2, _ = J(theta_plus)\n",
    "        \n",
    "        num_grad.append((loss2 - loss1) / (2*epsilon))\n",
    "        \n",
    "        perturb[i] = 0\n",
    "        \n",
    "    return np.array([num_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_nn_gradient(lambda_):\n",
    "\n",
    "    # Creates a small neural network in order to compare the implementation \n",
    "    # of back propagation with the numerical gradient approximation.\n",
    "    \n",
    "    input_layer_size = 3\n",
    "    hidden_layer_size = 5\n",
    "    num_labels = 3\n",
    "    m = 5\n",
    "    \n",
    "    X = initialize_theta(m, input_layer_size -1)\n",
    "    y = np.random.randint(0,3, m)\n",
    "    y_nn = np.zeros([m, num_labels])\n",
    "    for i in range(len(y)):\n",
    "        y_nn[i, y[i] - 1] = 1\n",
    "    \n",
    "    theta1 = initialize_theta(hidden_layer_size, input_layer_size)\n",
    "    theta2 = initialize_theta(num_labels, hidden_layer_size)\n",
    "    theta_vec = pack_theta(theta1, theta2)\n",
    "\n",
    "    cost, grad = cost_function(theta_vec, \n",
    "                               X, \n",
    "                               y_nn, \n",
    "                               input_layer_size, \n",
    "                               hidden_layer_size, \n",
    "                               num_labels, \n",
    "                               lambda_)\n",
    "\n",
    "    num_grad = compute_numerical_gradient(lambda theta: cost_function(theta, \n",
    "                                                        X, \n",
    "                                                        y_nn, \n",
    "                                                        input_layer_size, \n",
    "                                                        hidden_layer_size, \n",
    "                                                        num_labels, \n",
    "                                                        lambda_), theta_vec)\n",
    "    \n",
    "    diff = np.linalg.norm(num_grad - grad)/np.linalg.norm(num_grad + grad)\n",
    "    \n",
    "    print('Difference between numerical and computed gradients: {}'.format(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between numerical and computed gradients: 2.019370913255408e-09\n"
     ]
    }
   ],
   "source": [
    "# Check to make sure numerical gradients match up to those computed in the cost function\n",
    "# (difference should be very small).\n",
    "\n",
    "check_nn_gradient(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = len(X)\n",
    "input_layer = 400\n",
    "hidden_layer = 25\n",
    "num_labels = 10\n",
    "lambda_ = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert y to a 5000 X 10 matrix, where each row indicates membership to a digit class.\n",
    "\n",
    "y_nn = np.zeros([5000, 10])\n",
    "    \n",
    "for i in range(len(y)):\n",
    "    y_nn[i, y[i] - 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize weights.\n",
    "\n",
    "initial_theta1 = initialize_theta(L_in = 400, L_out=25)\n",
    "initial_theta2 = initialize_theta(L_in = 25, L_out = 10)\n",
    "\n",
    "initial_theta_vec = pack_theta(initial_theta1, initial_theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize cost function.\n",
    "\n",
    "result = op.minimize(fun=cost_function, \n",
    "                             x0=initial_theta_vec, \n",
    "                             args=(X, y_nn, input_layer, hidden_layer, num_labels, 3), \n",
    "                             method= 'TNC', \n",
    "                             jac=True, options = {'maxiter' : 400, 'disp': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimal_theta = result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimal_t1, optimal_t2 = unpack_theta(optimal_theta, input_layer, hidden_layer, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_NN(t1, t2, X, y):\n",
    "    \n",
    "    # Performs forward propagation and returns accuracy of predictions.\n",
    "    \n",
    "    a1 = add_bias_feature(X)\n",
    "    z2 = t1.dot(a1.T)\n",
    "    a2 = sigmoid(z2.T)\n",
    "    a2 = add_bias_feature(a2)\n",
    "    z3 = t2.dot(a2.T)\n",
    "    a3 = sigmoid(z3.T)\n",
    "    \n",
    "    predictions = np.argmax(a3, axis = 1) + 1\n",
    "    accuracy =  np.mean((predictions == y) * 1)\n",
    "    \n",
    "    return 'Training accuracy for neural network: {:0.2f}%'.format(accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training accuracy for neural network: 97.60%'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_NN(optimal_t1, optimal_t2, X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
